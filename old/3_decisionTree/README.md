# 결정 트리
- 데이터셋 안의 관측들을 가장 잘 표현하는 결정 트리를 생성하는 것이 목적


## 결정 트리 생성 알고리즘

### 재귀적 분기
- 하향식 접근법. 그리디 알고리즘
- 데이터셋을 하위 집합들로 재귀적으로 분기. 각 분기들에서 최저 비용 분기를 찾기 위해서 입력 속성과 비용 함수에 기반하여 분기를 평가
- 데이터셋을 하위 집합으로 분기시킬 때의 비용은 일반적으로 유사한 클래스 변수를 갖는 레코드들이 얼마나 같이 그룹지어지냐에 따라 계산됨

### 비용 함수
- 구축중인 결정 트리의 품질

#### 지니 불순도 (Gini Impurity)
- 랜덤 관측이 데이터셋의 클래스 변수 분포에 따라 분류된다고 할 때, 랜덤 관측의 오분류 가능성의 척도

- 다양한 클래스 변수들의 분포에 기반하여 데이터셋에 존재하는 노이즈의 양

- <img src="https://latex.codecogs.com/svg.latex?GiniImpurity=\Sigma_{i=1}^{J}p_i\cdot(1-p_i)=1-\Sigma _{i=1}^{J}p_i^2"/>

    - J개의 클래스, i로 레이블링된 관측 비율 (pi)

#### 정보 이득 (Information Gain)

- 물리학에서 랜덤 변수의 예측 불가능성을 나타내기 위해 사용
    - unbiased 동전은 예측 불가능성이 최대이므로 엔트로피가 1
    - biased 동전이고 H 확률이 100%이면 엔트로피는 0
- 특정 branch에서 클래스 변수의 예측 불가능성을 계산하는데 사용
- <img src="https://latex.codecogs.com/svg.latex?H(T)=-\Sigma_{i=1}^{J}p_i{log}_{2}{p_i}"/>

    - H(T) : 속성 T의 엔트로피
    - J : 데이터셋의 클래스 변수
    - pi : 데이터셋에서 클래스 i에 속하는 관측치의 비율

- <img src="https://latex.codecogs.com/svg.latex?IG(T,a)=H(T)-H(T|a)"/>

    - a : branch의 속성
    - H(T|a) : 자식들의 엔트로피 가중치 합

- 정보 이득이 가장 높은 속성을 사용하여 분기해야 최상의 결정 트리를 얻을 수 있음


## 결정 트리 유형 (CART 알고리즘)
- 분류 트리
    - 이산 값을 예측하는데 사용
    - 데이터셋의 클래스 변수가 이산 값을 가짐
- 회귀 트리
    - 실수를 예측하는데 사용

## 결정 트리 분기 중지

### 노드의 관측 수

- 관측 수가 미리 지정된 양보다 적을 때 분기에서 재귀를 중지하는 기준을 설정할 수 있음
- 보통 총 훈련 데이터의 5% 미만이 있는 branch에서는 재귀를 중지
- 각 노드에 하나의 데이터 포인트만 있도록 데이터를 과도하게 분기하면 결정 트리가 overfitting되어 정확히 분류되지 않을 수 있음

### 노드의 순도

- Gini Impurity에서 오류 가능성
- branch에서 하위 집합의 순도가 미리 규정된 임계값보다 큰 경우 branch의 분기를 중지

### 트리의 깊이

- 깊이 한계를 초과하는 branch의 분기를 중지

### 트리 전정 (가지치기)
- 일단 트리가 완전히 자라게 놔둠
- 전체 트리가 만들어진 후에 미리 규정된 부모 branch 불순도보다 더 작은 불순도를 갖는 branch를 제거

# 랜덤 포레스트 알고리즘

## 결정 트리의 단점

- 결정 트리는 비용 함수에 기반하여 속성에 관해 분기하도록 선택하는 알고리즘을 사용
    - 그리디 알고리즘을 사용하기 때문에 차적의 결정을 내리는 것이 최적의 결정 트리로 이끌지 여부를 조사하지 않음
- 훈련 데이터에 대해 overfitting 되는 경향이 있음 
    - 훈련에 사용된 데이터셋에 대해서는 정확하게 예측이 되나, overfitting 되기 때문에 이전에 본 적 없는 관측에 대해서는 정확하지 않을 수 있음 

## 랜덤 포레스트 알고리즘 

- 기존 결정 트리 알고리즘의 정확도를 향상

### 랜덤 포레스트 생성

1. 학습 데이터를 랜덤하게 하위 집합으로 나눔 (중복 허용)
1. 각각의 하위 집합에 대해 결정 트리를 생성 (독립적)

- overfitting 된 트리에 의존하지 않고 결정을 내리기 때문에 overfitting에 대한 문제가 해결됨
- 비용 함수에 기반하여 하나의 속성에 관해서만 분기하는 문제가 해결 됨
- 각각의 결정 트리는 훈련된 랜덤 샘플을 기반으로 서로 다른 속성에 기반하여 분기 결정을 내릴 수 있음

### 랜덤 포레스트 예측

- Bagging 접근법: 훈련 세트를 랜덤하게 부분 집합으로 나누고 여러 개의 머신러닝 모델을 훈련 (Bootstrap AGGregatING)
- 각 branch로부터 이벤트의 확률을 가져오고 투표 방식에 의해 예측
- overfitting이나 차적 결정을 내리는 트리에 의한 예측을 억제하는데 도움이 됨

# 그래디언트 부스팅 알고리즘

- 결정 트리 알고리즘의 단점을 해결
- 결정 트리들의 오차를 줄임으로서 여러 트리들을 순차적으로 훈련
    - 랜덤한 하위 집합에 기반하여 여러 트리를 학습시키는 랜덤 포레스트와 다름
- 결정 트리에서 표현하기 어려운 패턴들을 데이터에서 찾아내고 올바른 예측으로 이끄는 훈련 샘플에 더 큰 가중치를 부여
- 결정 트리의 오차를 낮춰주는 샘플들이 우선시 되는 훈련 데이터 하위 집합을 만듬
- 더 이상의 최적화로 이끌 수 있는 오류 패턴을 관찰 할 수 없을 때 프로세스 중단