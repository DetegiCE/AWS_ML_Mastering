{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "src_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터로부터 dataframe을 만듬\n",
    "housing_df = sql.read.csv(src_path + '/train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark에서는 입력 데이터셋에 모든 훈련 특징을 나타내는 숫자들의 벡터가 있는 단일 열이 필요\n",
    "#CountVectorizer에서 비슷한 기능\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "training_features = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']\n",
    "vector_assembler = VectorAssembler(inputCols=training_features, outputCol='features')\n",
    "df_with_features_vector = vector_assembler.transform(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|            features|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|[0.02729,0.0,7.07...|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_features_vector.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 프레임을 훈련용과 테스트용으로 분리\n",
    "train_df, test_df = df_with_features_vector.randomSplit([0.8, 0.2], seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#선형회귀기를 인스턴스화 하고 모델을 피팅\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "linear = LinearRegression(featuresCol='features', labelCol='medv')\n",
    "linear_model = linear.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측 값 구함\n",
    "predictions_df = linear_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|            features|        prediction|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...| 31.32098687370162|\n",
      "|  7|0.08829|12.5| 7.87|   0|0.524|6.012|66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|[0.08829,12.5,7.8...|22.702179170212247|\n",
      "| 12|0.11747|12.5| 7.87|   0|0.524|6.009|82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|[0.11747,12.5,7.8...| 21.19338299760661|\n",
      "| 21|1.25179| 0.0| 8.14|   0|0.538| 5.57|98.1|3.7979|  4|307|   21.0|376.57|21.02|13.6|[1.25179,0.0,8.14...|13.026016734659294|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6599665255387566"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regression Evaluator을 이용한 R2값 구하기\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol='medv', predictionCol='prediction',\n",
    "                               metricName='r2')\n",
    "evaluator.evaluate(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이프라인을 이용한 훈련\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "linear = LinearRegression(featuresCol='features', labelCol='medv')\n",
    "pipeline = Pipeline(stages=[vector_assembler, linear])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(linear.elasticNetParam, [0.01, 0.02, 0.05]) \\\n",
    "            .addGrid(linear.solver, ['normal', 'l-bfgs']) \\\n",
    "            .addGrid(linear.regParam, [0.4, 0.5, 0.6]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=10)\n",
    "\n",
    "optimized_model = crossval.fit(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aggregationDepth', 2),\n",
       " ('elasticNetParam', 0.05),\n",
       " ('epsilon', 1.35),\n",
       " ('featuresCol', 'features'),\n",
       " ('fitIntercept', True),\n",
       " ('labelCol', 'medv'),\n",
       " ('loss', 'squaredError'),\n",
       " ('maxIter', 100),\n",
       " ('predictionCol', 'prediction'),\n",
       " ('regParam', 0.4),\n",
       " ('solver', 'l-bfgs'),\n",
       " ('standardization', True),\n",
       " ('tol', 1e-06)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k.name, v) for (k, v) in optimized_model.bestModel.stages[1].extractParamMap().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6923323960037392,\n",
       " 0.6923959499732874,\n",
       " 0.6923502431104525,\n",
       " 0.6923330103241978,\n",
       " 0.692396061514483,\n",
       " 0.692349562517069,\n",
       " 0.6924196977934123,\n",
       " 0.6924732106548477,\n",
       " 0.6924168566051652,\n",
       " 0.6924201717986757,\n",
       " 0.6924732826313917,\n",
       " 0.6924160898881038,\n",
       " 0.692678590380535,\n",
       " 0.6926286783262435,\n",
       " 0.6923471608287199,\n",
       " 0.6926789836249727,\n",
       " 0.6926276596010446,\n",
       " 0.6923446609885943]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810905508925078"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, evaluation_df = housing_df.randomSplit([0.8, 0.2], seed=17)\n",
    "evaluator.evaluate(optimized_model.transform(evaluation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
