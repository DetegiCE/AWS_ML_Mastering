# 선형 회귀법
- 회귀 알고리즘은 non-binary 예측 작업에 사용 가능 하기 때문에 중요한 알고리즘
- 예측 대상인 종속 변수와 독립 변수의 벡터 간의 관계를 모델링
- 독립 변수 벡터와 예측 대상 종속 변수 사이에 선형적인 관계가 있다고 가정
- 선형 회귀는 회귀기 변수들에 기반하여 종속 변수를 예측하는 것이 목표인 경우들에 적용 가능
- 두 변수 간에 정확한 선형 관계는 없지만, 추세를 나타내는 근사선을 만들 수는 있음. 선형 회귀 모델링 알고리즘의 목표는 이 근사선의 오차를 최소화 하는 것

- <img src="https://latex.codecogs.com/svg.latex?y={\beta}_i x + {\beta}_0 + \epsilon"/>

## 선형 회귀의 추정 

- 선형 모델을 데이터에 맞추려면(fit) 먼저 선형 모델이 데이터에 얼마나 잘 맞는지 결정해야 함

### 선형 최소 제곱 추정

- 주어진 데이터를 기반으로 파라미터를 추정하는데 사용하는 추정 방식

- <img src="https://latex.codecogs.com/svg.latex?min_{\beta_0, \beta_1} : \Sigma _{x=1} ^{n} [ y_i - (\beta_0 + \beta_1 x_i ) ] ^2"/>

- LLS는 beta_0과 beta_1의 값을 추정하여 최적 솔루션을 얻음

### 최대 우도 추정

- 선형 회귀의 파라미터들을 추정하는데 사용되는 모델

- 관측된 데이터셋을 다시 만들 가능성이 최대인 파라미터 값을 예측할 수 있는 확률 모델

- <img src="https://latex.codecogs.com/svg.latex?l(parameter|data) = \Sigma _{i=1} ^n f(data_i | parameters)"/>

### 경사 하강

- 함수를 최소화 하는데 사용

- 초기 값에서 시작해 오차 함수를 최소화 하는 방향으로 반복적으로 파라미터 값을 이동

- 모든 샘플에 대한 경사 관찰을 모은 후 한 번만 하강하는 batch GD와 각 샘플마다 경사 관찰 후 하강하는 stochastic GD가 있음


## 회귀 모델 평가

### 평균 절대 오차

- 두 벡터 사이의 평균 오차

- 모델 예측의 오차를 명확하게 해석해주기 때문에 많이 사용됨

### 평균 제곱 오차

- 오차 값의 제곱의 평균

- 오차가 매우 작은 경우에 유용

### R 제곱

- R-제곱 점수 또는 결정 계수라 부름

- 회귀 모델에서 독립 변수들로부터 예측 가능한 종속 변수의 분산 비율을 측정

- 완벽한 회귀기에서 최상의 값은 1.0

## 장단점

### 장점

- 훈련 시간
    - 데이터셋을 간단한 선형 모델로 표현하는 단순성
- 모델을 조사해서 전체 모델을 결정하는데 어떤 변수가 영향을 미치는지 이해할 수 있음
- 문제가 간단하고 예측에 적은 변수가 사용되는 경우에 사용

### 단점

- 데이터셋의 복잡성이 증가함에 따라 데이터에 노이즈가 많아 심각한 오차 발생 가능
- 종속 변수가 회귀기들과 선형 관계에 있다고 과감히 가정
    - 이 사실이 아니라면 선형 회귀 알고리즘이 데이터에 잘 맞추지 못할 수 있음
    - 이를 해결한 2차 회귀가 있으나, 모델이 복잡해져 훈련 시간이 늘어남